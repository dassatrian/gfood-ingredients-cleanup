{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal: have a script to run from start to finish that will call ingredients endpoint and upload ingredients to Cosmos DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in databases\n",
    "food_details = pd.read_excel('raw_data/Release 1 - Food details file.xlsx')\n",
    "\n",
    "food_nutrients_excel = pd.ExcelFile('raw_data/Release 1 - Food nutrient database.xlsx')\n",
    "food_nutrients = pd.read_excel(food_nutrients_excel,'All solids & liquids per 100g')\n",
    "\n",
    "food_measures = pd.read_excel('raw_data/Release 1 - Measures file.xlsx')\n",
    "food_recipe = pd.read_excel('raw_data/Release 1 - Recipe file.xlsx')\n",
    "\n",
    "food_emissions = pd.read_excel('raw_data/emissions.xlsx')\n",
    "food_categories = pd.read_excel('raw_data/categories.xlsx')\n",
    "\n",
    "initial_classification = pd.read_csv('raw_data/initial_classification.csv')\n",
    "\n",
    "# to store all of the food product classifications, now we can drop Classification Name from the food database\n",
    "food_classification = food_details[['Classification ID', 'Classification Name']].copy()\n",
    "food_classification.drop_duplicates(inplace = True)\n",
    "\n",
    "# need to fix up the merged columns (ie. make the \"Unnamed\" column match the previous column name) and remove the units row (make a part of column name)\n",
    "regex = re.compile(r'Unnamed') # regex to find the \"Unnamed\" columns\n",
    "\n",
    "old_columns = list(food_nutrients.columns) # list of existing columns\n",
    "units = food_nutrients[0:1].values[0] # list of the unit row\n",
    "new_columns = [] # empty list to store the new column values\n",
    "\n",
    "for each in range(len(old_columns)):\n",
    "    if re.search(regex, food_nutrients.columns[each]): # ie. find the column that is \"unnamed\"\n",
    "        try:\n",
    "            new_columns.append(old_columns[each-1] + ' (in ' + units[each] + ')') # take the previous column name and add units\n",
    "        except:\n",
    "            new_columns.append(old_columns[each-1]) # if you can't add the units, just keep the column\n",
    "    else: # ie. column is not \"unnamed\"\n",
    "        try:\n",
    "            new_columns.append(old_columns[each] + ' (in ' + units[each] + ')')\n",
    "        except:\n",
    "            new_columns.append(old_columns[each])\n",
    "    \n",
    "food_nutrients.rename(columns = dict(zip(old_columns, new_columns)), inplace = True) # rename the columns\n",
    "food_nutrients.drop(0, axis = 0, inplace = True) # drop the unit row\n",
    "food_nutrients.reset_index(drop = True, inplace = True) # reset the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information about our datasets\n",
    "* food_details # information about the ingredient\n",
    "* food_nutrients # nutritional information about the ingredient\n",
    "* food_measures # information about the measures of ingredients\n",
    "* food_recipe # a list of recipes (useless)\n",
    "* food_emissions # ghg emissions table \n",
    "* food_categories # HSR calculation\n",
    "* food_classification # food classifications which came with the dataset\n",
    "\n",
    "## Joining on the Initial Classification (so that we can calculate HSR rating and emissions)\n",
    "HSR calculation and GHG emissions relies on having a particular classification to a group. Since our open data sets don't have these groups, I have had to manually create an initial_classification file, which I have done at a broad category level, and will apply down to the individual ingredients. Not accurate, but quick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_details = food_details.merge(initial_classification, on = 'Classification ID', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining nutritional data and food_details so we can calculate HSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_details = food_details.merge(food_nutrients, on = 'Public Food Key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating HSR for Ingredients\n",
    "Now that food_details have emission_id, hsr_id as well as all nutritional data we can calculate HSR rating for each ingredient (where available), and also GHG emissions (approximate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in hsr tables\n",
    "hsr_table_1 = pd.read_csv('raw_data/hsr_table_1.csv')\n",
    "hsr_table_2 = pd.read_csv('raw_data/hsr_table_2.csv')\n",
    "hsr_table_3 = pd.read_csv('raw_data/hsr_table_3.csv')\n",
    "hsr_table_4 = pd.read_csv('raw_data/hsr_table_4.csv')\n",
    "\n",
    "# function to find the closest value in a list\n",
    "def get_closest_value(arr, target):\n",
    "    n = len(arr)\n",
    "    left = 0\n",
    "    right = n - 1\n",
    "    mid = 0\n",
    "\n",
    "    # edge case - last or above all\n",
    "    if target >= arr[n - 1]:\n",
    "        return arr[n - 1]\n",
    "    # edge case - first or below all\n",
    "    if target <= arr[0]:\n",
    "        return arr[0]\n",
    "    # BSearch solution: Time & Space: Log(N)\n",
    "\n",
    "    while left < right:\n",
    "        mid = (left + right) // 2  # find the mid\n",
    "        if target < arr[mid]:\n",
    "            right = mid\n",
    "        elif target > arr[mid]:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            return arr[mid]\n",
    "\n",
    "    if target < arr[mid]:\n",
    "        return find_closest(arr[mid - 1], arr[mid], target)\n",
    "    else:\n",
    "        return find_closest(arr[mid], arr[mid + 1], target)\n",
    "\n",
    "\n",
    "# findClosest\n",
    "# We find the closest by taking the difference\n",
    "# between the target and both values. It assumes\n",
    "# that val2 is greater than val1 and target lies\n",
    "# between these two. \n",
    "def find_closest(val1, val2, target):\n",
    "    return val2 if target - val1 >= val2 - target else val1\n",
    "\n",
    "# function to find index using get_closest_value\n",
    "# could probably optimise get_closest_value to just return the index, but maybe later\n",
    "def index_finder(arr, target):\n",
    "    if target > get_closest_value(arr, target):\n",
    "         return arr.index(get_closest_value(arr,target))\n",
    "    else:\n",
    "        return arr.index(get_closest_value(arr, target))-1\n",
    "    \n",
    "def hsr(index):\n",
    "    # Step 1) Get the category\n",
    "    category_id = food_details['hsr_id'][index]\n",
    "\n",
    "    # Step 2) Get the required columns\n",
    "    energy = food_details['Energy, with dietary fibre (in kJ)'][index]\n",
    "    tsfa = food_details['Total saturated fatty acids (in g)'][index]\n",
    "    sugars = food_details['Total sugars (in g)'][index]\n",
    "    sodium = food_details['Sodium (Na) (in mg)'][index]\n",
    "    protein = food_details['Protein (in g)'][index]\n",
    "    fibre = food_details['Total dietary fibre (in g)'][index]\n",
    "\n",
    "    # Step 3) Calculate HSR Baseline Points\n",
    "    # read tables in as lists\n",
    "    if food_details['hsr_id'][index] == '1' or food_details['hsr_id'][index] == '1D' or food_details['hsr_id'][index] == '2' or food_details['hsr_id'][index] == '2D':\n",
    "        hsr_energy = list(hsr_table_1['Energy content (kJ) per 100g or 100mL'])\n",
    "        hsr_tsfa = list(hsr_table_1['Saturated fatty acids (g) per 100g or 100mL'])\n",
    "        hsr_sugars = list(hsr_table_1['Total sugars (g) per 100g or 100mL'])\n",
    "        hsr_sodium = list(hsr_table_1['Sodium (mg) per 100g or 100mL'])\n",
    "    elif food_details['hsr_id'][index] == '3' or food_details['hsr_id'][index] == '3D': \n",
    "        hsr_energy = list(hsr_table_2['Energy content (kJ) per 100g or 100mL'])\n",
    "        hsr_tsfa = list(hsr_table_2['Saturated fatty acids (g) per 100g or 100mL'])\n",
    "        hsr_sugars = list(hsr_table_2['Total sugars (g) per 100g or 100mL'])\n",
    "        hsr_sodium = list(hsr_table_2['Sodium (mg) per 100g or 100mL'])\n",
    "    else:\n",
    "        return numpy.nan\n",
    "    \n",
    "\n",
    "    # remove nans\n",
    "    hsr_energy = [x for x in hsr_energy if str(x) != 'nan']\n",
    "    hsr_tsfa = [x for x in hsr_tsfa if str(x) != 'nan']\n",
    "    hsr_sugars = [x for x in hsr_sugars if str(x) != 'nan']\n",
    "    hsr_sodium = [x for x in hsr_sodium if str(x) != 'nan']\n",
    "\n",
    "    # need some way of quick of getting baseline points based on the table\n",
    "    baseline_points = []\n",
    "    baseline_points.append(index_finder(hsr_energy, energy))\n",
    "    baseline_points.append(index_finder(hsr_tsfa, tsfa))\n",
    "    baseline_points.append(index_finder(hsr_sugars, sugars))\n",
    "    baseline_points.append(index_finder(hsr_sodium, sodium))\n",
    "\n",
    "    # PICK MAX INDEX!!\n",
    "    baseline_points.sort() \n",
    "\n",
    "    # Step 4) Modifying Points (P and F only since we don't have V points)\n",
    "    # read in p and f tables\n",
    "    hsr_p = list(hsr_table_3['Protein (g) per 100g or 100mL'])\n",
    "    hsr_f = list(hsr_table_3['Dietary fibre (g) per 100g or 100mL'])\n",
    "\n",
    "    p_points = index_finder(hsr_p, protein)\n",
    "    f_points = index_finder(hsr_f, fibre)\n",
    "\n",
    "    #Step 5) Calculate Final HSR Score\n",
    "    final_hsr_score = baseline_points[-1] - p_points - f_points\n",
    "    hsr = hsr_table_4['Health Star Rating'][list(hsr_table_4[category_id]).index(get_closest_value(list(hsr_table_4[category_id]), final_hsr_score))]\n",
    "    \n",
    "    return hsr\n",
    "\n",
    "ingredient_hsr = []\n",
    "worked = 0\n",
    "not_worked = 0\n",
    "\n",
    "for each in range(len(food_details)):\n",
    "    try:\n",
    "        ingredient_hsr.append(hsr(each))\n",
    "        worked +=1\n",
    "    except:\n",
    "        ingredient_hsr.append(numpy.nan)\n",
    "        not_worked +=1 \n",
    "        \n",
    "# need to write back to food_details \n",
    "food_details['hsr'] = ingredient_hsr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining to get GHG emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_details = food_details.merge(food_emissions, on = ['emission_id'], how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering down 'food name' and dropping duplicates\n",
    "At the moment, the food_details table has all the information we need about particular ingredients,however the 'Name' column has a lot of 'duplicates' of ingredients, based on not much needed information.\n",
    "\n",
    "The goal here is to find by which comma can we split such that the difference in Energy (ie. the range between highest and lowest value), is appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping by name: Energy, with dietary fibre (in kJ)  mm_diff    265.771831\n",
      "dtype: float64\n",
      "Grouping by name1: Energy, with dietary fibre (in kJ)  mm_diff    104.449184\n",
      "dtype: float64\n",
      "Grouping by name2: Energy, with dietary fibre (in kJ)  mm_diff    57.403153\n",
      "dtype: float64\n",
      "Grouping by name3: Energy, with dietary fibre (in kJ)  mm_diff    24.626109\n",
      "dtype: float64\n",
      "Grouping by name4: Energy, with dietary fibre (in kJ)  mm_diff    22.076253\n",
      "dtype: float64\n",
      "Grouping by name5: Energy, with dietary fibre (in kJ)  mm_diff    1.171569\n",
      "dtype: float64\n",
      "Grouping by name6: Energy, with dietary fibre (in kJ)  mm_diff    0.0\n",
      "dtype: float64\n",
      "Grouping by name7: Energy, with dietary fibre (in kJ)  mm_diff    0.0\n",
      "dtype: float64\n",
      "Grouping by name8: Energy, with dietary fibre (in kJ)  mm_diff    0.0\n",
      "dtype: float64\n",
      "Grouping by name9: Energy, with dietary fibre (in kJ)  mm_diff    0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# find the max number of separations in \"Food Name\" column\n",
    "commas = []\n",
    "for each in food_details[\"Food Name\"]:\n",
    "    commas.append(len(each.split(',')))\n",
    "\n",
    "max(commas)\n",
    "\n",
    "# split food\n",
    "name_split = food_details[\"Food Name\"].str.split(\",\", n = 9, expand = True)\n",
    "\n",
    "food_details[\"name\"] = name_split[0]\n",
    "food_details[\"name1\"] = name_split[1]\n",
    "food_details[\"name2\"] = name_split[2]\n",
    "food_details[\"name3\"] = name_split[3]\n",
    "food_details[\"name4\"] = name_split[4]\n",
    "food_details[\"name5\"] = name_split[5]\n",
    "food_details[\"name6\"] = name_split[6]\n",
    "food_details[\"name7\"] = name_split[7]\n",
    "food_details[\"name8\"] = name_split[8]\n",
    "food_details[\"name9\"] = name_split[9]\n",
    "\n",
    "# function to calculate range\n",
    "def mm_diff(x):\n",
    "           return np.max(x) - np.min(x)\n",
    "    \n",
    "names = food_details[['name', 'name1', 'name2', 'name3', 'name4', 'name5', 'name6', 'name7', 'name8', 'name9','Energy, with dietary fibre (in kJ)']]\n",
    "print('Grouping by name: ' + str(names.groupby('name').agg([mm_diff]).mean()))\n",
    "print('Grouping by name1: ' + str(names.groupby(['name', 'name1']).agg([mm_diff]).mean()))\n",
    "print('Grouping by name2: ' + str(names.groupby(['name', 'name1', 'name2']).agg([mm_diff]).mean()))\n",
    "print('Grouping by name3: ' + str(names.groupby(['name', 'name1', 'name2', 'name3']).agg([mm_diff]).mean()))\n",
    "print('Grouping by name4: ' + str(names.groupby(['name', 'name1', 'name2', 'name3', 'name4']).agg([mm_diff]).mean()))\n",
    "print('Grouping by name5: ' + str(names.groupby(['name', 'name1', 'name2', 'name3', 'name4', 'name5']).agg([mm_diff]).mean()))\n",
    "print('Grouping by name6: ' + str(names.groupby(['name', 'name1', 'name2', 'name3', 'name4', 'name5', 'name6']).agg([mm_diff]).mean()))\n",
    "print('Grouping by name7: ' + str(names.groupby(['name', 'name1', 'name2', 'name3', 'name4', 'name5', 'name6', 'name7']).agg([mm_diff]).mean()))\n",
    "print('Grouping by name8: ' + str(names.groupby(['name', 'name1', 'name2', 'name3', 'name4', 'name5', 'name6', 'name7', 'name8']).agg([mm_diff]).mean()))\n",
    "print('Grouping by name9: ' + str(names.groupby(['name', 'name1', 'name2', 'name3', 'name4', 'name5', 'name6', 'name7', 'name8', 'name9']).agg([mm_diff]).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see from the analysis done above, if we only consider 1 comma, then our potential error will be approximately 100kj (which equates to around 20 calories)\n",
    "# So, we should drop duplicates, based on name and name1 combination\n",
    "food_details.drop_duplicates(subset = ['name', 'name1'], inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the appropriate data model and posting to the API\n",
    "We should only send the columns that we need from our dataset:\n",
    "\n",
    "* name\n",
    "* name1 (to be called type)\n",
    "* Description\n",
    "* Energy, with dietary fibre (in kJ)\n",
    "* hsr\n",
    "* hsr_id\n",
    "* GHG Emissions (kg CO2eq/FU) (Mean)\n",
    "* emissions_id\n",
    "* Protein (in g)\n",
    "* Sodium (Na) (in mg)\n",
    "* Total sugars (in g)\n",
    "* Total saturated fatty acids (in g)\n",
    "* Total dietary fibre (in g)\n",
    "* Total Fat (in g)\n",
    "* Available carbohydrate, without sugar alcohols (in g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to clean up the Nones so it can be handled nicely by the API\n",
    "food_details = food_details.replace({np.nan: None}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup food_details before we send to API\n",
    "food_details.rename(columns={'Food Name':'name' ,\\\n",
    "                            'name1': 'ingredient_type',\\\n",
    "                            'Description': 'description',\\\n",
    "                            'Energy, with dietary fibre (in kJ)': 'energy',\\\n",
    "                            'Protein (in g)': 'protein',\\\n",
    "                            'Sodium (Na) (in mg)': 'sodium',\\\n",
    "                            'Total sugars (in g)': 'sugar',\\\n",
    "                            'Total saturated fatty acids (in g)': 'saturated_fat',\\\n",
    "                            'Total dietary fibre (in g)': 'fibre',\\\n",
    "                            'Total Fat (in g)': 'fat',\\\n",
    "                            'Available carbohydrate, without sugar alcohols (in g)': 'carbohydrate'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = food_details.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredient = {\"name\" : row.name, \\\n",
    "\"ingredient_type\" : row.ingredient_type, \\\n",
    "\"description\" : row.description, \\\n",
    "\"energy\" : row.energy, \\\n",
    "\"hsr\" : row.hsr, \\\n",
    "\"hsr_id\" : row.hsr_id, \\\n",
    "\"emission_id\" : row.emission_id, \\\n",
    "\"protein\" : row.protein, \\\n",
    "\"sodium\" : row.sodium, \\\n",
    "\"sugar\" : row.sugar, \\\n",
    "\"saturated_fat\" : row.saturated_fat, \\\n",
    "\"fibre\" : row.fibre, \\\n",
    "\"fat\" : row.fat, \\\n",
    "\"carbohydrate\" : carbohydrate}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
